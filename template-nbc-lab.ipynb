{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lti3WORYZsmK-webhook"
      },
      "source": [
        "# Naive Bayes Classifier\n",
        "\n",
        "Welcome to your next lab! You will build Naive Bayes Classifier.\n",
        "\n",
        "You will classify spam/ham messages.\n",
        "\n",
        "**You will learn to:**\n",
        "- Build the general architecture of a learning algorithm with OOP in mind:\n",
        "    - Helper functions\n",
        "        - Preprocessing data\n",
        "        - Calculate prior probs for classes\n",
        "        - Calculate likelihood probs of each word in a class\n",
        "    - Main Model Class\n",
        "        - Training\n",
        "        - Prediction \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nJpGK7JQHoq"
      },
      "source": [
        "> **Important note:** Before submission make sure that you **didn't add or delete any notebook cells**. Otherwise your work may not be accepted by the validator!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRv9oWNGQHor"
      },
      "source": [
        "## 0 - Download data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1GrD0GIFQHos"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wget in /Users/victoriaboichenko/opt/anaconda3/lib/python3.8/site-packages (3.2)\n",
            "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'spam (2).csv'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install wget\n",
        "import wget\n",
        "wget.download('https://dru.fra1.digitaloceanspaces.com/DS_Fundamentals/datasets/04_supervised_learning/Naive_Bayes_Classifier/spam.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F86OV8dwZsmL"
      },
      "source": [
        "## 1 - Packages ##\n",
        "\n",
        "First, let's run the cell below to import all the packages that you will need during this assignment.\n",
        "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
        "- [pandas](https://pandas.pydata.org/) is a library providing a convenient work with data.\n",
        "- [re](https://docs.python.org/3/library/re.html) is for regex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eEoj1DOwZsmM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5Wy6bYpZsmO"
      },
      "source": [
        "## 2 - Overview of the Problem set ##\n",
        "\n",
        "**Problem Statement**: You are given a dataset  containing:\n",
        "\n",
        "* a training set of `m_train` examples\n",
        "* a test set of `m_test` examples\n",
        "* each example is a message that belongs to a particular class: ham or spam.\n",
        "\n",
        "Let's get more familiar with the dataset. Load the data by running the following code.\n",
        "\n",
        "We won't divide our data to features(X) and target(Y) here, because we need to preprocess it in a special way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "DtUaewx1ZsmP"
      },
      "outputs": [],
      "source": [
        "# Loading the data \n",
        "\n",
        "def load_data():\n",
        "    df = pd.read_csv('spam.csv', encoding='latin-1')\n",
        "    df_for_tests = df.head()\n",
        "    \n",
        "    idx = np.arange(df.shape[0])\n",
        "    np.random.shuffle(idx)\n",
        "\n",
        "    train_set_size = int(df.shape[0] * 0.8)\n",
        "\n",
        "    train_set = df.loc[idx[:train_set_size]]\n",
        "    test_set = df.loc[idx[train_set_size:]]\n",
        "    \n",
        "    return train_set, test_set, df_for_tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "ae4YTbLtZsmR"
      },
      "outputs": [],
      "source": [
        "train_set, test_set, df_for_tests = load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fxIXA7FZsmU"
      },
      "source": [
        "## 3 - Naive Bayes Classifier\n",
        "**Mathematical expression of the algorithm**:\n",
        "\n",
        "\n",
        "This algorithm is based on Bayes' theorem:\n",
        "$$  \\begin{equation}\n",
        "    P(A_{j}\\hspace{0.1cm}\\rvert\\hspace{0.1cm}x_{1},\\dots,x_{n}) = \\frac{P(x_{1},\\dots,x_{n}\\hspace{0.1cm}\\rvert\\hspace{0.1cm}A_{j})P(A_{j})}{P(x_{1},\\dots,x_{n})} \n",
        "    \\end{equation}$$\n",
        "    \n",
        "Ignoring denominator (because it stays the same for all cases):\n",
        "\n",
        "$$ \\begin{equation}\n",
        "    P(A_{j})P(x_{1},\\dots,x_{n}\\hspace{0.1cm}\\rvert\\hspace{0.1cm}A_{j}) = P(A_{j}, x_{1},\\dots,x_{n}) = \\\\\n",
        "  \\hspace{1cm} = P(x_{1}\\hspace{0.1cm}\\rvert\\hspace{0.1cm}x_{2},\\dots,x_{n}, A_{j})P(x_{2}\\hspace{0.1cm}\\rvert\\hspace{0.1cm}x_{3}, \\dots ,x_{n}, A_{j})\\dots P(x_{n-1}\\hspace{0.1cm}\\rvert\\hspace{0.1cm}x_{n}, A_{j}) \\approx \\\\\n",
        "  \\hspace{1cm}\n",
        "  \\end{equation}$$\n",
        "By making an assumption that the $x_{i}$ are conditionally independent of each other:\n",
        "$$ \\begin{equation} \\approx P(A_{j}) \\prod_{i=1}^{n} P(x_{i}\\hspace{0.1cm}\\rvert\\hspace{0.1cm}A_{j})\n",
        "   \\end{equation}$$\n",
        "   \n",
        "We can calculate the probability, if we know the prior probability:\n",
        "\n",
        "$$ \\begin{equation}\n",
        "    y^{*} = \\operatorname*{arg\\,max}_{j} \\big(P(A_{j}) \\prod_{i=1}^{n} P(x_{i}\\hspace{0.1cm}\\rvert\\hspace{0.1cm}A_{j})\\big)\n",
        "   \\end{equation}$$\n",
        "   \n",
        "   \n",
        "Due to floating point underflow, the above is usually replaced with the numerically tractable expression:\n",
        "\n",
        "$$ \\begin{equation}\n",
        "    y^{*} = \\operatorname*{arg\\,max}_{j} \\big( \\ln(P(A_{j})) + \\sum_{i=1}^{n} \\ln(P(x_{i}\\hspace{0.1cm}\\rvert\\hspace{0.1cm}A_{j})) \\big)\n",
        "   \\end{equation}$$\n",
        "   \n",
        "For more consistent knowledge of the NBC algorithm, we highly recommend you to read [this Stanford University article](https://web.stanford.edu/~jurafsky/slp3/4.pdf)\n",
        "\n",
        "**Training the Naive Bayes Classifier**:\n",
        "\n",
        "How can we find the probabilities $\\ln(P(A_{j}))$ and $\\ln(P(x_{i}\\hspace{0.1cm}\\rvert\\hspace{0.1cm}A_{j}))$ ? We'll simply use the frequencies in the data. For the class prior $P(A_{j})$ we find what percentage of the messages in our training set are in each class $A_{j}$:\n",
        "\n",
        "$$ \\begin{equation}\n",
        "    \\ln(P(A_{j})) = \\ln\\big(\\frac{N_{A_{j}}}{N}\\big)\n",
        "    \\tag{1}\n",
        "   \\end{equation}$$\n",
        "\n",
        "where $N_{A_{j}}$ is the number of messages in our training data with class $A_{j}$ and $N$ be the total number of messages.\n",
        "\n",
        "In $P(x_{i}\\hspace{0.1cm}\\rvert\\hspace{0.1cm}A_{j})$ we just compute as the fraction of times the word $x_{i}$ appears among all words in all messages of class $A_{j}$:\n",
        "\n",
        "$$ \\begin{equation}\n",
        "    \\ln(P(x_{i}\\hspace{0.1cm}\\rvert\\hspace{0.1cm}A_{j})) = \\ln\\big(\\frac{ N_{x_i, A_j}}{|A_{j}|} \\big)\n",
        "    \\tag{2}\n",
        "   \\end{equation}\n",
        "$$\n",
        "\n",
        "where $N_{x_i, A_j}$ is number of times word $x_{i}$ appears in messages from class $A_{j}$ and $|A_{j}|$ - total count of all words in class $A_{j}$.\n",
        "\n",
        "   \n",
        "#### Laplace smoothing\n",
        "\n",
        "In statistics, additive smoothing, also called Laplace smoothing, or Lidstone smoothing, is a technique that is used to smooth categorical data. Given an observation \n",
        "$\\begin{equation}\n",
        "    message = (x_{1}\\, \\dots \\,x_{n})\n",
        " \\end{equation}$, a \"smoothed\" version of the data gives the estimator:\n",
        "\n",
        "\n",
        "$$ \\begin{equation}\n",
        "    \\ln(P(x_{i}\\hspace{0.1cm}\\rvert\\hspace{0.1cm}A_{j})) = \\ln\\big(\\frac{ N_{x_i, A_j} + \\alpha}{ |A_{j}| +  \\alpha * |V|} \\big)\n",
        "    \\tag{3}\n",
        "   \\end{equation}\n",
        "$$\n",
        "\n",
        "where the pseudocount \n",
        "$\\begin{equation}\n",
        "    \\alpha > 0\n",
        " \\end{equation}$ is the smoothing parameter (\n",
        "$\\begin{equation}\n",
        "    \\alpha = 0\n",
        " \\end{equation}$ corresponds to no smoothing) and $V$ is a vocabulary, which consists of the union of all the words in all classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9mF4dTVZsmU"
      },
      "source": [
        "### 3.1 - Preprocessing the data \n",
        "\n",
        "Our data consists of different messages. Messages contain some excess symbols, which don't affect the content of the text, but add noise to the data.\n",
        "For example: \"Does not \\\\operate 66.7 after  & lt;# & gt;  or what\". \n",
        "\n",
        "Let's clean our data and leave only letters a-z, A-Z, numbers 0-9 and cast all letters to lowercase, replace double to n spaces with just one space, remove trailing spaces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OZDyrOMZZsmV"
      },
      "outputs": [],
      "source": [
        "# Clean the data\n",
        "\n",
        "def clean_data(message):\n",
        "    \n",
        "    \"\"\" \n",
        "    Returns string which consists of message words\n",
        "    \n",
        "    Argument:\n",
        "    message -- message from dataset; \n",
        "        type(message) -> <class 'str'>\n",
        "    \n",
        "    Returns:\n",
        "    result -- cleaned message, which contains only letters a-z, and numbers 0-9, with only one space between words;\n",
        "        type(clean_data(message)) -> <class 'str'>\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    message_in = message.lower()\n",
        "    result = ''\n",
        "    for i in range(len(message_in)):\n",
        "        \n",
        "        if message_in[i].isalnum() or message_in[i].isspace():\n",
        "            result += message_in[i]\n",
        "        else: \n",
        "            result += \" \"\n",
        "    \n",
        "    # Remove multiple spaces\n",
        "    result = re.sub(r'\\s\\s+', ' ', result)\n",
        "    return result\n",
        "    ### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "r96K1BnlZsmX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cleaned:  doesn t get how to operate 66 7 after it lt gt won t or what \n",
            "cleaned:  o k lar7i double check wif da hair dresser already he said 77 88 5 wun cut v short question std txt rate t c s\n"
          ]
        }
      ],
      "source": [
        "sentence_1 = 'Doesn\\'t get, how{to}% \\\\operate+66.7 :after[it]\"\" & lt;# & gt; won\\'t `or(what)'\n",
        "sentence_2 = 'O\\]k,.lar7i$double{} check wif*& da! hair: [dresser;   ..already He SaID-77.88.5 wun cut v short question(std txt rate)T&C\\'s'\n",
        "print('cleaned: ',clean_data(sentence_1))\n",
        "print('cleaned: ',clean_data(sentence_2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-A6HAOtZsmY"
      },
      "source": [
        "**Expected Output**: \n",
        "\n",
        "<table style=\"width:70%\">\n",
        "    <tr>\n",
        "        <td><b>cleaned:</b></td>\n",
        "       <td> doesn t get how to operate 66 7 after it lt gt won t or what </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td><b>cleaned:</b></td>\n",
        "       <td> o k lar7i double check wif da hair dresser already he said 77 88 5 wun cut v short question std txt rate t c s </td>\n",
        "    </tr>\n",
        "    \n",
        "\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsLYrKmtZsmZ"
      },
      "source": [
        "Now let's clean each sentence and split data on features(X) and target(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "gEFD2wuFZsma"
      },
      "outputs": [],
      "source": [
        "# Preparation data for model\n",
        "\n",
        "def prep_for_model(train_set, test_set):\n",
        "    \n",
        "    \"\"\" \n",
        "    Returns arrays of train/test features(words) and train/test targets(labels)\n",
        "    \n",
        "    Arguments:\n",
        "    train_set -- train dataset, which consists of train messages and labels; \n",
        "        type(train_set) -> pandas.core.frame.DataFrame\n",
        "    test_set -- test dataset, which consists of test messages and labels; \n",
        "        type(train_set) -> pandas.core.frame.DataFrame\n",
        "    \n",
        "    Returns:\n",
        "    train_set_x -- array which contains lists of words of each cleaned train message; \n",
        "        (type(train_set_x) ->numpy.ndarray[list[str]], train_set_x.shape = (num_messages,))\n",
        "    train_set_y -- array of train labels (names of classes), \n",
        "        (type(train_set_y) -> numpy.ndarray, train_set_y.shape = (num_messages,))\n",
        "    test_set_x -- array which contains lists of words of each cleaned test message;\n",
        "        (type(test_set_x) numpy.ndarray[list[str]], test_set_x.shape = (num_messages,)\n",
        "    test_set_y -- array of test labels (names of classes), \n",
        "        (type(test_set_y) -> numpy.ndarray, test_set_y.shape = (num_messages,))\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # your train set features\n",
        "    def prep_message(x):\n",
        "        x = clean_data(x)\n",
        "        return x.split()\n",
        "   \n",
        "    train_set_y = train_set.iloc[:,0].to_numpy()\n",
        "    train_set_x = train_set.iloc[:,1].apply(prep_message).to_numpy()\n",
        "\n",
        "    test_set_y = test_set.iloc[:,0].to_numpy()\n",
        "    test_set_x = test_set.iloc[:,1].apply(prep_message).to_numpy()\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return train_set_x, train_set_y, test_set_x, test_set_y\n",
        "\n",
        "\n",
        "train_set_x, train_set_y, test_set_x, test_set_y = prep_for_model(train_set, test_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "bnNLInEBZsmc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ham ['go', 'until', 'jurong', 'point', 'crazy', 'available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'there', 'got', 'amore', 'wat']\n",
            "ham ['u', 'dun', 'say', 'so', 'early', 'hor', 'u', 'c', 'already', 'then', 'say']\n"
          ]
        }
      ],
      "source": [
        "a1, a2, b1, b2 = prep_for_model(df_for_tests.head(3), df_for_tests.tail(2))\n",
        "print(a2[0], a1[0])\n",
        "print(b2[0], b1[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gm4MoiogZsmf"
      },
      "source": [
        "**Expected Output**: \n",
        "\n",
        "<table style=\"width:40%\">\n",
        "    <tr>\n",
        "        <td><b>ham:</b></td>\n",
        "       <td> ['go', 'until', 'jurong', 'point', 'crazy', 'available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'there', 'got', 'amore', 'wat'] </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td><b>ham:</b></td>\n",
        "       <td> ['u', 'dun', 'say', 'so', 'early', 'hor', 'u', 'c', 'already', 'then', 'say']\n",
        " </td>\n",
        "    </tr>\n",
        "\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn_Ag9ZQZsmg"
      },
      "source": [
        "Now let's check words in each category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "olXobIliZsmg"
      },
      "outputs": [],
      "source": [
        "# Check words in categories\n",
        "\n",
        "def categories_words(x_train, y_train):\n",
        "    \n",
        "    \"\"\"\n",
        "    Returns arrays of features(words) in each category and in both categories\n",
        "    \n",
        "    Arguments:\n",
        "    x_train -- array which contains lists of words of each cleaned train message; \n",
        "        (type(x_train) -> numpy.ndarray[list[str]], x_train.shape = (num_messages,))\n",
        "    \n",
        "    Returns:\n",
        "    all_words_list -- array of all words in both categories;\n",
        "        (type(all_words_list) -> numpy.ndarray[str], all_words_list.shape = (num_words,))\n",
        "    ham_words_list -- array of words in 'ham' class;\n",
        "        (type(ham_words_list) -> numpy.ndarray[str], ham_words_list.shape = (num_words,))\n",
        "    spam_words_list -- array of words in 'spam' class;\n",
        "        (type(spam_words_list) -> numpy.ndarray[str], spam_words_list.shape = (num_words,))        \n",
        "    \"\"\"\n",
        "    all_words_list = []\n",
        "    ham_words_list = []\n",
        "    spam_words_list = []\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # all_words_list = sum(x_train, [])\n",
        "    for i in range(len(y_train)):\n",
        "        if y_train[i] == 'ham':\n",
        "            ham_words_list += x_train[i]\n",
        "        else: \n",
        "            spam_words_list += x_train[i]\n",
        "    all_words_list = ham_words_list + spam_words_list     \n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return all_words_list, ham_words_list, spam_words_list\n",
        "\n",
        "all_words_list_a1, ham_words_list_a1, spam_words_list_a1 = categories_words(a1, a2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "zCkJB-7TZsmj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "first five \"ham\" words of a1:  ['go', 'until', 'jurong', 'point', 'crazy']\n",
            "first five \"ham\" words of a1:  ['go', 'until', 'jurong', 'point', 'crazy', 'available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'there', 'got', 'amore', 'wat', 'ok', 'lar', 'joking', 'wif', 'u', 'oni']\n"
          ]
        }
      ],
      "source": [
        "print('first five \"ham\" words of a1: ', ham_words_list_a1[:5])\n",
        "print('first five \"ham\" words of a1: ', ham_words_list_a1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phUVohsyZsmk"
      },
      "source": [
        "**Expected Output**: \n",
        "\n",
        "<table style=\"width:40%\">\n",
        "    <tr>\n",
        "        <td><b>first five \"ham\" words of a1:</b></td>\n",
        "       <td> ['go' 'until' 'jurong' 'point' 'crazy'] </td>\n",
        "    </tr>\n",
        "\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0Izv7t7gCAL"
      },
      "source": [
        "Let's calculate probability of each word in a category. Here you need to use (3) formula."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "eo_VKmc9QLsF"
      },
      "outputs": [],
      "source": [
        "# Calculate log probability of each word in a category using smoothing\n",
        "\n",
        "def calc_words_probs_of_category(category_words_list, all_words_list, alpha):\n",
        "\n",
        "    \"\"\"\n",
        "    Returns a dict of log probs words belonging to a category\n",
        "    \n",
        "    Arguments:\n",
        "    category_words_list -- array of words in category class (\"ham\" or \"spam\");\n",
        "        (type(category_words_arr) -> numpy.ndarray[str], category_words_list.shape = (num_words,))\n",
        "    all_words_list -- array of all words in both categories;\n",
        "        (type(all_words_list) -> numpy.ndarray[str], all_words_list.shape = (num_words,))\n",
        "    alpha -- int number. The smoothing coeficient.\n",
        "    \n",
        "    Returns:\n",
        "    category_words_dict -- dictionary containing log probability of each word in a category.\n",
        "        type(category_words_dict) -> dict\n",
        "    \"\"\"\n",
        "    category_words_dict = {}\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    count_all_words = len(np.unique(np.array(all_words_list)))\n",
        "    count_category_words = len(category_words_list)\n",
        "\n",
        "    # Make an array of frequencies \n",
        "    (unique, counts) = np.unique(category_words_list, return_counts=True)\n",
        "    frequencies = np.asarray((unique, counts))\n",
        "    # print(frequencies)\n",
        "    \n",
        "\n",
        "    for word in all_words_list:\n",
        "        # print(word)\n",
        "        \n",
        "        has_word = word in frequencies[0] \n",
        "        # print\n",
        "        # print(has_word)\n",
        "        index = int(np.where(frequencies[0] == word)[0]) if has_word else -1\n",
        "        count_of_word = float(frequencies[1][index]) if has_word else 0\n",
        "\n",
        "        prob = (count_of_word + alpha)/(alpha * count_all_words + count_category_words)\n",
        "        category_words_dict[word] = np.log(prob)\n",
        "\n",
        "    # print(type(category_words_dict))\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return category_words_dict\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "Y1eccDrUQL1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The log prob of the word “87121” from the \"ham\" category in a1: -4.3694478524670215\n",
            "\n",
            "The log prob of the word “87121” from the \"spam\" category in a1: -3.7612001156935624\n"
          ]
        }
      ],
      "source": [
        "ham_words_dict_a1 = calc_words_probs_of_category(ham_words_list_a1, all_words_list_a1, alpha=1)\n",
        "print('The log prob of the word “87121” from the \"ham\" category in a1:', ham_words_dict_a1['87121'])\n",
        "\n",
        "spam_words_dict_a1 = calc_words_probs_of_category(spam_words_list_a1, all_words_list_a1, alpha=1)\n",
        "print('\\nThe log prob of the word “87121” from the \"spam\" category in a1:', spam_words_dict_a1['87121'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1v6KZE_RI8l"
      },
      "source": [
        "**Expected Output**: \n",
        "\n",
        "<table style=\"width:70%\">\n",
        "    <tr>\n",
        "        <td><b>The log prob of the word “87121” from the \"ham\" category in a1:</b></td>\n",
        "       <td>-4.3694478524670215</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td><b>The log prob of the word “87121” from the \"spam\" category in a1:</b></td>\n",
        "       <td>-3.7612001156935624</td>\n",
        "    </tr>\n",
        "    \n",
        "\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P5kBQrTgi2g"
      },
      "source": [
        "Сalculating the prior probability for each class category. Here you need to use (1) formula."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "Bz2-UTgEQL-B"
      },
      "outputs": [],
      "source": [
        "# Calculate prior log probability of each category\n",
        "\n",
        "def calc_prior_category_prob(y_train):\n",
        "\n",
        "    \"\"\"\n",
        "    Returns prior probabilities of each category \n",
        "    \n",
        "    Arguments:\n",
        "    y_train -- array which contains list of labels of each message; \n",
        "        (type(y_train) -> numpy.ndarray[list[str]], x_train.shape = (num_messages,))\n",
        "\n",
        "    Returns:\n",
        "    prior_ham_prob -- float number. Prior log probability of \"ham\" category\n",
        "    prior_spam_prob -- float number. Prior log probability of \"spam\" category   \n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    count_all_labels = len(y_train)\n",
        "\n",
        "    (unique, counts) = np.unique(y_train, return_counts=True)\n",
        "    frequencies = np.asarray((unique, counts))\n",
        "\n",
        "    probs = {}\n",
        "    for i in frequencies[0]:\n",
        "        index = int(np.where(frequencies[0] == i)[0])\n",
        "        count_label = float(frequencies[1][index])\n",
        "        prob = np.log(count_label/count_all_labels)\n",
        "        probs[i] = prob\n",
        "\n",
        "    prior_ham_prob, prior_spam_prob = probs.values()\n",
        "\n",
        "    return prior_ham_prob, prior_spam_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "uC9AAWPRQcsE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prior log probability of \"ham\" category in a2:  -0.40546510810816444\n",
            "Prior log probability of \"spam\" category in a2:  -1.0986122886681098\n"
          ]
        }
      ],
      "source": [
        "print('Prior log probability of \"ham\" category in a2: ', calc_prior_category_prob(a2)[0])\n",
        "print('Prior log probability of \"spam\" category in a2: ', calc_prior_category_prob(a2)[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMIF-d_vSHep"
      },
      "source": [
        "**Expected Output**: \n",
        "\n",
        "<table style=\"width:70%\">\n",
        "    <tr>\n",
        "        <td><b>Prior log probability of \"ham\" category in a2:</b></td>\n",
        "       <td>-0.40546510810816</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td><b>Prior log probability of \"spam\" category in a2:</b></td>\n",
        "       <td>-1.0986122886681</td>\n",
        "    </tr>\n",
        "    \n",
        "\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AARnBVXGZsmk"
      },
      "source": [
        "### 3.2 Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "LO7_8FEKZsmm"
      },
      "outputs": [],
      "source": [
        "class Naive_Bayes(object):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    -----------\n",
        "    alpha: int\n",
        "        The smoothing coeficient.\n",
        "    \"\"\"\n",
        "    def __init__(self, alpha):\n",
        "        self.alpha = alpha\n",
        "        \n",
        "        self.train_set_x = None\n",
        "        self.train_set_y = None\n",
        "        \n",
        "        self.all_words_list = []\n",
        "        self.ham_words_list = []\n",
        "        self.spam_words_list = []\n",
        "\n",
        "        self.ham_words_dict = {}\n",
        "        self.spam_words_dict = {}\n",
        "\n",
        "        self.prior_ham_prob = None\n",
        "        self.prior_spam_prob = None\n",
        "\n",
        "        #You are allowed to create new attributes and methods (but it's not a necessary)\n",
        "    \n",
        "    def fit(self, train_set_x, train_set_y):\n",
        "        \n",
        "        # Generate all_words_list, ham_words_list, spam_words_list using function 'categories_words'; \n",
        "        # Calculate probability of each word in both categories using function 'calc_words_probs_of_category';\n",
        "        # Calculate prior probability of each category using function 'calc_prior_category_prob'.\n",
        "        ### START CODE HERE ### \n",
        "        self.train_set_x = train_set_x\n",
        "        self.train_set_y = train_set_y\n",
        "\n",
        "        self.all_words_list, self.ham_words_list, self.spam_words_list = categories_words(self.train_set_x, self.train_set_y)\n",
        "\n",
        "        self.ham_words_dict = calc_words_probs_of_category(self.ham_words_list, self.all_words_list, self.alpha)\n",
        "\n",
        "        self.spam_words_dict = calc_words_probs_of_category(self.spam_words_list, self.all_words_list, self.alpha)\n",
        "\n",
        "        self.prior_ham_prob, self.prior_spam_prob = calc_prior_category_prob(self.train_set_y)\n",
        "        \n",
        "        ### END CODE HERE ### \n",
        "        \n",
        "    def predict(self, test_set_x):\n",
        "      \n",
        "        # Calculate probabilities of belonging to ham and spam category \n",
        "        # Compare these probabilities and choose the max\n",
        "        # Return list of predicted labels for test set; type(prediction) -> list, len(prediction) = len(test_set_y)\n",
        "        ### START CODE HERE ###\n",
        "        prediction = []\n",
        "\n",
        "        ham_probs = calc_words_probs_of_category(self.ham_words_list, test_set_x, self.alpha)\n",
        "\n",
        "        spam_probs = calc_words_probs_of_category(self.spam_words_list, test_set_x, self.alpha)\n",
        "\n",
        "        for i in range(len(test_set_x)):\n",
        "            if ham_probs[i] >= spam_probs[i]:\n",
        "                prediction.append('ham')\n",
        "            else :\n",
        "                prediction.append('spam')\n",
        "        ### END CODE HERE ### \n",
        "        return prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVU0yzP1Zsmo"
      },
      "source": [
        "## 4 - Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIiilw0KZsmo"
      },
      "source": [
        "First of all, we should define a smoothing coeficient (`alpha`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "two70VtTZsmp"
      },
      "outputs": [],
      "source": [
        "a = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0FVH2nMZsmr"
      },
      "source": [
        "Now we can initialize our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "lFOgwc0eZsmr"
      },
      "outputs": [],
      "source": [
        "model = Naive_Bayes(alpha=a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBs3q36OZsmu"
      },
      "source": [
        "Let's train our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "dh3_xodbZsmv"
      },
      "outputs": [],
      "source": [
        "model.fit(train_set_x, train_set_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MruvACv2Zsmw"
      },
      "source": [
        "## 5 - Making predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "0-zUCJU_Zsmy"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "shape mismatch: objects cannot be broadcast to a single shape",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-136-d7013be362fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-127-7431f3836152>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, test_set_x)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mham_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_words_probs_of_category\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mham_words_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mspam_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_words_probs_of_category\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspam_words_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-118-3d9719608047>\u001b[0m in \u001b[0;36mcalc_words_probs_of_category\u001b[0;34m(category_words_list, all_words_list, alpha)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# print(word)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mhas_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfrequencies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;31m# print\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# print(has_word)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shape mismatch: objects cannot be broadcast to a single shape"
          ]
        }
      ],
      "source": [
        "y_predictions = model.predict(test_set_x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNA1wZRAZsmy"
      },
      "source": [
        "Let's calculate accuracy (accuracy of model must be >0.95):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBFDdUdkZsm0"
      },
      "outputs": [],
      "source": [
        "actual = list(test_set_y)\n",
        "accuracy = (y_predictions == test_set_y).mean()\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-Wk8eTRZsm1"
      },
      "source": [
        "## 6 - Conclusion\n",
        "As we can see, our model fits well the hypothesis function to the data.\n",
        "\n",
        "#### What's next:\n",
        "1. Try experimenting with the `alpha` to see how this affects the model you have built.\n",
        "2. Compare the results you have obtained with the `sklearn.naive_bayes.MultinomialNB` model.\n",
        "3. Try this model in the wild! Select your favorite dataset [here](https://www.kaggle.com/datasets?sortBy=hottest&group=public&page=1&pageSize=20&size=small&filetype=all&license=all&tagids=13303) and play with it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaLpZWBiQHo-"
      },
      "source": [
        "##### Naive Bayes Classifier Done!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vw9jSsdiQHo-"
      },
      "source": [
        "##### Make sure that you didn't add or delete any notebook cells. Otherwise your work may not be accepted by the validator!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "template_temp.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "6aeb3db7f206cdb2f424a4a314ffd92e673031d39a950de50feb53b6f9ef9ed1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
